## âš¡ Inference II: Quantization

**Quantization** reduces the numerical precision of model weights (and sometimes activations) to make inference **faster and cheaper**.

Instead of storing weights as:

```
float32 (32 bits)
```

we use:

```
float16 (16 bits)
int8  (8 bits)
int4  (4 bits)
```

---

# ğŸ§  Why Quantization Works

Neural networks donâ€™t need ultra-precise numbers to function well.

Example:

```
0.123456789  â†’  0.1235
```

Tiny rounding error â†’ almost no impact on output.

So we trade:

| Precision â†“ | Memory â†“ | Speed â†‘ |
| ----------- | -------- | ------- |

---

# ğŸ“¦ What Gets Quantized?

### 1ï¸âƒ£ Weights (most common)

Huge memory savings.

### 2ï¸âƒ£ Activations (optional)

More aggressive optimization.

### 3ï¸âƒ£ KV-cache (advanced optimization)

Very helpful for long-context inference.

---

# ğŸ”¢ Memory Impact Example

Suppose a 7B parameter model.

### FP16:

```
7B Ã— 2 bytes â‰ˆ 14 GB
```

### INT8:

```
7B Ã— 1 byte â‰ˆ 7 GB
```

### INT4:

```
7B Ã— 0.5 byte â‰ˆ 3.5 GB
```

Massive savings.

---

# ğŸ— How It Works (Conceptually)

We map floating-point values to integers:

### Step 1 â€” Find range

```
min_weight, max_weight
```

### Step 2 â€” Scale

```
float_value â‰ˆ scale Ã— int_value
```

Where:

```
scale = (max - min) / 255   (for int8)
```

So at runtime:

* Multiply integers
* Rescale back

Modern GPUs are very efficient at this.

---

# ğŸ¯ Types of Quantization

### ğŸ”¹ Post-Training Quantization (PTQ)

* Quantize after training
* Fast, simple
* Slight quality drop possible

Common in deployment.

---

### ğŸ”¹ Quantization-Aware Training (QAT)

* Simulate quantization during training
* Better accuracy retention
* More complex

---

# ğŸ”¥ Popular Quantization Methods for LLMs

* **Dynamic quantization**
* **Static quantization**
* **GPTQ**
* **AWQ**
* **BitsAndBytes (LLM.int8, 4-bit)**

Each balances:

* Speed
* Memory
* Accuracy

---

# âš– Tradeoffs

| Benefit                     | Cost                 |
| --------------------------- | -------------------- |
| Lower memory                | Slight accuracy loss |
| Faster inference            | Possible instability |
| Larger models on small GPUs | Extra engineering    |

---

# ğŸ§  Simple Mental Model

Think of it like compressing a high-resolution image:

* 4K â†’ 1080p
  Still clear, but much smaller.

---

# ğŸš€ Why It Matters for Inference

Quantization enables:

* Running 7Bâ€“13B models on consumer GPUs
* Lower cloud costs
* Higher batch sizes
* Faster token throughput

---

If you'd like, I can next explain:

* Quantization + KV-cache interaction
* Why 4-bit works surprisingly well
* Per-channel vs per-tensor quantization
* Quantization math in matrix multiplication (C#-friendly explanation)
