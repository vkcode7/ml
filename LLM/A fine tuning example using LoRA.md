Below is a **clean end-to-end example** of how you would LoRA-fine-tune an OpenAI model in Python using a corpus of ~10,000 SEC 10-K documents from U.S. Securities and Exchange Commission (SEC.gov).

> âš ï¸ Important reality:
> OpenAIâ€™s hosted fine-tuning abstracts the underlying PEFT method (e.g., LoRA). You donâ€™t manually implement LoRA layers â€” the platform handles parameter-efficient fine-tuning internally.
> So you fine-tune via API; LoRA happens under the hood.

---

# ğŸ¯ Goal

Fine-tune a model so it:

* Understands financial reporting language
* Answers questions grounded in 10-K filings
* Summarizes risk factors, MD&A, etc.
* Produces structured financial analysis

---

# ğŸ§  Step 0 â€” Decide the Strategy

You generally **donâ€™t fine-tune directly on raw 10-K text**.

Better pipeline:

```
Raw 10-K docs
   â†“
Chunk
   â†“
Create instruction/QA pairs
   â†“
SFT fine-tuning
```

Why?

Fine-tuning teaches:

* Style
* Task behavior
* Structured reasoning

RAG handles:

* Factual grounding
* Specific filings

In production, you'd combine both.

---

# ğŸ§± Step 1 â€” Prepare Training Data

You need JSONL in chat format:

```json
{"messages":[
  {"role":"system","content":"You are a financial analyst."},
  {"role":"user","content":"Summarize the risk factors for Apple in 2023."},
  {"role":"assistant","content":"Apple's 2023 10-K highlights risks including supply chain dependence, regulatory scrutiny, foreign exchange volatility..."}
]}
```

---

## ğŸ”¹ Example Data Preparation Script

```python
import json
import glob

def create_training_examples():
    examples = []
    
    for file in glob.glob("sec_docs/*.txt"):
        with open(file, "r") as f:
            text = f.read()
        
        # Example transformation (simplified)
        prompt = "Summarize the key risk factors from this 10-K."
        response = summarize_risk_section(text)  # your custom parser
        
        example = {
            "messages": [
                {"role": "system", "content": "You are a CFA-level financial analyst."},
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
        }
        
        examples.append(example)

    return examples


def save_jsonl(data, filename):
    with open(filename, "w") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")


training_data = create_training_examples()
save_jsonl(training_data, "financial_finetune.jsonl")
```

Youâ€™d generate thousands of such instruction-style examples.

---

# ğŸš€ Step 2 â€” Upload File to OpenAI

```python
from openai import OpenAI

client = OpenAI()

file = client.files.create(
    file=open("financial_finetune.jsonl", "rb"),
    purpose="fine-tune"
)

print(file.id)
```

---

# ğŸ”¥ Step 3 â€” Create Fine-Tuning Job

```python
job = client.fine_tuning.jobs.create(
    training_file=file.id,
    model="gpt-4o-mini-2024-07-18",  # example fine-tunable model
    hyperparameters={
        "n_epochs": 3
    }
)

print(job.id)
```

---

# ğŸ“Š Step 4 â€” Monitor Training

```python
status = client.fine_tuning.jobs.retrieve(job.id)
print(status.status)
```

When complete, youâ€™ll receive:

```
fine_tuned_model: ft:gpt-4o-mini:your-org:financial-analyst:abc123
```

---

# ğŸ§  Step 5 â€” Use Your Fine-Tuned Model

```python
response = client.chat.completions.create(
    model="ft:gpt-4o-mini:your-org:financial-analyst:abc123",
    messages=[
        {"role": "system", "content": "You are a financial analyst."},
        {"role": "user", "content": "Analyze Tesla's liquidity risks."}
    ]
)

print(response.choices[0].message.content)
```

---

# ğŸ“ˆ What LoRA Is Doing Behind the Scenes

Internally:

* Base model weights frozen
* Small low-rank matrices added
* Only small subset trained
* Memory-efficient updates

You donâ€™t manually configure rank â€” OpenAI manages that.

---

# ğŸ— Production Architecture (Recommended)

For financial use cases:

```
Fine-tuned model (style + reasoning)
         +
RAG (actual filings retrieval)
```

Flow:

```
User Question
   â†“
Vector search over 10-K chunks
   â†“
Inject relevant sections
   â†“
Fine-tuned model response
```

This prevents hallucination.

---

# âš– When Fine-Tuning Is Worth It

Use fine-tuning if you need:

* Consistent financial tone
* Structured outputs (e.g., risk tables)
* Domain-specific reasoning style
* Regulatory-safe language

Do NOT fine-tune just to â€œstoreâ€ 10-K facts.

Thatâ€™s what RAG is for.

---

# ğŸ§  Practical Advice (Finance Domain)

For 10,000 SEC docs:

1. Build RAG first
2. Then fine-tune on:

   * Structured financial analysis examples
   * Risk summarization
   * Earnings call Q&A
   * Ratio interpretation

This gives best ROI.

---

# ğŸ’° Cost Reality

Fine-tuning cost depends on:

* Tokens in dataset
* Number of epochs
* Model size

10,000 filings â†’ likely millions of tokens
You may want:

* Section-level sampling
* Not full-document training

---

# ğŸ¯ End-to-End Summary

```
SEC 10-K corpus
   â†“
Chunk + convert to instruction pairs
   â†“
Upload JSONL
   â†“
Create fine-tuning job
   â†“
Deploy fine-tuned model
   â†“
Combine with RAG in production
```

---

If you'd like, I can next:

* Show a RAG + fine-tuned hybrid architecture
* Estimate token cost for 10,000 documents
* Show C# version of deployment (since you use .NET)


# Show how to auto-generate training pairs from filings

Below is a **practical, production-style pipeline** to auto-generate instruction/response training pairs from 10-K filings obtained from U.S. Securities and Exchange Commission.

Weâ€™ll turn raw filings into high-quality SFT training data.

---

# ğŸ¯ Goal

Convert:

```
Raw 10-K text (10000 docs)
```

Into:

```
Instruction â†’ High-quality assistant answer
```



For fine-tuning.

---

# ğŸ§  High-Level Strategy

We do NOT train on raw filings.

We:

1. Parse sections
2. Chunk intelligently
3. Generate synthetic Q&A pairs
4. Filter for quality
5. Export JSONL for fine-tuning

---

# ğŸ§± Step 1 â€” Parse Structured Sections

10-Ks follow predictable structure:

* Item 1A â€“ Risk Factors
* Item 7 â€“ MD&A
* Item 8 â€“ Financial Statements
* etc.

We extract by regex or XML parsing.

```python
import re

def extract_section(text, section_title):
    pattern = rf"{section_title}(.+?)(Item\s+\d+[A-Z]?)"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    return match.group(1).strip() if match else None
```

Example:

```python
risk_section = extract_section(doc_text, "Item 1A")
mda_section = extract_section(doc_text, "Item 7")
```

---

# ğŸ§© Step 2 â€” Chunk the Text

Chunk size should be:

* 800â€“1500 tokens
* Section-aware
* Sentence-boundary aligned

```python
from nltk.tokenize import sent_tokenize

def chunk_text(text, max_chars=4000):
    sentences = sent_tokenize(text)
    chunks = []
    current = ""

    for s in sentences:
        if len(current) + len(s) < max_chars:
            current += " " + s
        else:
            chunks.append(current.strip())
            current = s

    if current:
        chunks.append(current.strip())

    return chunks
```

---

# ğŸ§  Step 3 â€” Auto-Generate Training Pairs

We now use a strong base model to generate Q&A pairs.

Prompt template:

```python
GEN_PROMPT = """
You are a CFA-level financial analyst.

From the following 10-K excerpt, generate:
1. One analytical question an investor might ask.
2. A detailed, professional answer grounded only in the text.
3. A short structured summary (bullet points).

Text:
{chunk}
"""
```

---

## Python Generation Code

```python
from openai import OpenAI

client = OpenAI()

def generate_pair(chunk):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a financial analyst."},
            {"role": "user", "content": GEN_PROMPT.format(chunk=chunk)}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content
```

---

# ğŸ§¾ Step 4 â€” Convert to Fine-Tune Format

We structure it as:

```python
import json

def format_for_sft(question, answer):
    return {
        "messages": [
            {"role": "system", "content": "You are a CFA-level financial analyst."},
            {"role": "user", "content": question},
            {"role": "assistant", "content": answer}
        ]
    }
```

You may need to parse the generated text into Q/A fields.

---

# ğŸ§ª Step 5 â€” Quality Filtering (Critical)

We should filter:

* Very short answers
* Generic responses
* Hallucinated content
* Repetitive samples

Simple heuristics:

```python
def quality_filter(answer):
    if len(answer) < 300:
        return False
    if "not mentioned" in answer.lower():
        return False
    return True
```

Advanced filtering:

* Run a second model pass to check grounding
* Check if answer quotes text
* Use semantic similarity vs source chunk

---

# ğŸ— Step 6 â€” Pipeline Over 10,000 Documents

```python
import glob

training_data = []

for file in glob.glob("sec_docs/*.txt"):
    with open(file, "r") as f:
        text = f.read()

    risk_section = extract_section(text, "Item 1A")
    if not risk_section:
        continue

    chunks = chunk_text(risk_section)

    for chunk in chunks:
        generated = generate_pair(chunk)
        question, answer = parse_generated(generated)

        if quality_filter(answer):
            training_data.append(format_for_sft(question, answer))

save_jsonl(training_data, "financial_training.jsonl")
```

---

# ğŸ“Š Expected Yield

From 10,000 filings:

* ~5 sections per filing
* ~5 chunks per section
* ~1 QA per chunk

â†’ Potentially 200k+ training examples

You may downsample to 20â€“50k high-quality examples.

---

# ğŸ§  What This Achieves

You are teaching the model:

* Financial reasoning style
* Risk analysis patterns
* Professional tone
* Structured outputs
* SEC-style language

NOT memorization.

---

# âš ï¸ Important: Avoid Data Leakage

Do NOT include:

* Exact company-specific facts in evaluation
* Future-dated filings
* Sensitive proprietary overlays

---

# ğŸ”¥ Advanced Improvements

### 1ï¸âƒ£ Generate Multiple Question Types

For each chunk:

* Risk analysis question
* Ratio interpretation
* Liquidity analysis
* Forward-looking uncertainty

---

### 2ï¸âƒ£ Add Structured Output Examples

Train on outputs like:

```
Liquidity Risk:
Operational Risk:
Regulatory Risk:
Market Risk:
```

---

### 3ï¸âƒ£ Add Negative Examples (for DPO later)

Generate:

* Weak answer
* Strong answer
* Label preferred

Useful for alignment training.

---

# ğŸ¯ Best Production Strategy

Do BOTH:

* Fine-tune for style and reasoning
* Use RAG for factual grounding

Because:

Fine-tuning â‰  factual database

---

# ğŸ§  Final Mental Model

Raw filings â†’ Structured chunks â†’ Synthetic analyst Q&A â†’ Filter â†’ JSONL â†’ Fine-tune

Youâ€™re teaching the model:

> â€œHow to think like a financial analystâ€

---

If you'd like, I can next:

* Show how to estimate token cost for 10,000 filings
* Show how to combine this with RAG
* Show how to detect hallucinations during generation


# Show evaluation metrics for financial fine-tuning

Excellent â€” this is where most fine-tuning projects fail.

For financial 10-K fine-tuning, evaluation must measure:

1. **Financial correctness**
2. **Grounding to filings**
3. **Analytical quality**
4. **Hallucination risk**
5. **Style consistency**
6. **General capability retention**

Below is a production-grade evaluation framework.

---

# ğŸ¯ 1ï¸âƒ£ Core Evaluation Categories

| Category           | What It Measures                 | Why It Matters            |
| ------------------ | -------------------------------- | ------------------------- |
| Factual Accuracy   | Correct financial interpretation | Avoid regulatory risk     |
| Groundedness       | Answer supported by source text  | Prevent hallucinations    |
| Analytical Depth   | True financial reasoning         | Not surface summarization |
| Consistency        | Stable structured output         | Enterprise usability      |
| General Capability | Model didnâ€™t degrade             | Avoid overfitting         |

---

# ğŸ§  2ï¸âƒ£ Offline Automatic Metrics

These are scalable.

---

## ğŸ”¹ A. Exactness / Overlap (ROUGE / BLEU)

Useful only for:

* Summarization tasks
* Structured templates

But limited for reasoning.

Use lightly.

---

## ğŸ”¹ B. Semantic Similarity (Embedding-Based)

Compare generated answer to:

* Gold reference answer
* Source chunk

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

Metric:

```
sim(generated, reference)
```

Better than ROUGE for finance language.

---

## ğŸ”¹ C. Groundedness Score (Critical)

We check:

> Does every major claim appear in source text?

Prompt a model:

```text
Given the source text and the model answer,
identify unsupported claims.
Return a hallucination score from 0â€“1.
```

Metric:

```
Hallucination Rate = (# unsupported claims) / (total claims)
```

This is extremely important for SEC data.

---

# ğŸ¦ 3ï¸âƒ£ Financial Reasoning Evaluation

We must test real financial logic.

Create evaluation tasks like:

* â€œExplain how interest rate increases impact this companyâ€™s debt profile.â€
* â€œAssess liquidity risk using balance sheet excerpts.â€
* â€œCompare current year vs prior year revenue drivers.â€

Have:

* Gold expert-written answers
* Or rubric-based scoring

---

## ğŸ”¹ Rubric Scoring (Very Effective)

Define scoring dimensions:

| Dimension              | Score 1â€“5 |
| ---------------------- | --------- |
| Correct interpretation |           |
| Risk identification    |           |
| Causal reasoning       |           |
| Financial terminology  |           |
| Regulatory awareness   |           |

You can automate rubric scoring via LLM evaluator.

Example evaluator prompt:

```text
Score the answer from 1-5 for:
- Financial correctness
- Depth of analysis
- Use of financial terminology
Return JSON only.
```

---

# ğŸ”¬ 4ï¸âƒ£ Hallucination Detection Metrics

### ğŸ”¹ Unsupported Fact Rate

Claims not present in filing.

### ğŸ”¹ Fabricated Numeric Rate

Invented numbers not in source.

Extremely important for financial systems.

---

# ğŸ“Š 5ï¸âƒ£ Regression Testing (Before vs After FT)

Always compare:

| Metric                      | Base Model | Fine-Tuned |
| --------------------------- | ---------- | ---------- |
| Financial QA accuracy       |            |            |
| Hallucination rate          |            |            |
| Structured format adherence |            |            |
| General knowledge QA        |            |            |

We want:

* Domain improvement
* No general collapse

---

# ğŸ§ª 6ï¸âƒ£ Hold-Out Filing Evaluation

Never evaluate on training filings.

Use:

* Different companies
* Different years
* Different industries

Test:

* Cross-sector generalization
* New regulatory language

---

# ğŸ§  7ï¸âƒ£ Style & Formatting Consistency

Check:

* Structured sections present
* Bullet formatting correct
* Risk categories consistently labeled

Metric example:

```
Format Adherence Rate =
(# responses following template) / (total responses)
```

---

# ğŸ“ˆ 8ï¸âƒ£ RAG-Aware Evaluation

If using RAG:

Measure:

* Retrieval accuracy (is correct section retrieved?)
* Answer grounded in retrieved text?
* Context utilization rate

Metric:

```
Context Utilization Score =
% of answer sentences that cite retrieved chunk
```

---

# ğŸ§  9ï¸âƒ£ Catastrophic Forgetting Check

Test general tasks:

* Basic math
* Common reasoning
* Non-financial QA

We want no degradation.

---

# ğŸ”¥ 10ï¸âƒ£ Human Expert Evaluation (Gold Standard)

For financial applications:

Have CFA / CPA review sample outputs.

Score:

* Regulatory safety
* Investment-grade reasoning
* Risk nuance
* Disclosure awareness

Even 100 human-reviewed samples is powerful.

---

# ğŸ“Š Example Full Evaluation Dashboard

| Metric                       | Target   |
| ---------------------------- | -------- |
| Financial QA accuracy        | >85%     |
| Hallucination rate           | <5%      |
| Unsupported numeric claims   | 0%       |
| Format adherence             | >95%     |
| Cross-sector generalization  | Stable   |
| General knowledge regression | <2% drop |

---

# ğŸ¯ What Matters Most in Finance

1. Hallucination rate
2. Fabricated numbers
3. Misinterpretation of risk
4. Overconfident tone

These are regulatory liabilities.

---

# ğŸ§  Final Mental Model

Financial fine-tuning evaluation is not:

> â€œDoes it sound smart?â€

It is:

> â€œIs it grounded, correct, structured, and safe?â€

---

If you'd like, I can next:

* Show an automated evaluation pipeline in Python
* Show how to build a financial LLM benchmark dataset
* Show how to quantify hallucination mathematically
* Show how to detect numeric fabrication
* Show a full evaluation workflow integrated with CI/CD

# Esimate Token cost for 10K documents

Letâ€™s estimate token cost for **10,000 SEC filings (10-K)** for fine-tuning and/or inference.

Weâ€™ll assume:

* Average 10-K length: **70,000â€“120,000 words**
* Rough rule: **1 token â‰ˆ 0.75 words**
* So: **1 word â‰ˆ 1.3 tokens**

---

# 1ï¸âƒ£ Estimate Tokens Per Document

### Conservative estimate

* 80,000 words per filing
* Tokens â‰ˆ 80,000 Ã— 1.3
* â‰ˆ **104,000 tokens per document**

### Total for 10,000 documents

```
104,000 Ã— 10,000 = 1,040,000,000 tokens
```

ğŸ‘‰ **~1.0â€“1.2 billion tokens total**

Thatâ€™s realistic for full 10-Ks.

---

# 2ï¸âƒ£ But You Rarely Train on Full Raw Documents

In practice you:

* Chunk into 1â€“4k token segments
* Extract Q&A pairs
* Filter boilerplate
* Remove tables / exhibits

Typical reduction:

| Strategy                  | Effective Tokens |
| ------------------------- | ---------------- |
| Raw full filings          | ~1B tokens       |
| Remove boilerplate        | ~600â€“800M        |
| Q/A extraction only       | ~100â€“300M        |
| Targeted risk & MD&A only | ~150â€“400M        |

---

# 3ï¸âƒ£ Training Cost Estimate (Example)

Letâ€™s assume:

* 300M tokens after cleaning
* 3 training epochs
* Effective tokens seen = 900M tokens

Now assume hypothetical fine-tuning price:

```
$X per 1M tokens
```

If cost = $8 per 1M tokens (example pricing class):

```
900M / 1M Ã— $8
= 900 Ã— 8
= $7,200
```

If cost = $4 per 1M:

```
= $3,600
```

If cost = $2 per 1M:

```
= $1,800
```

So realistic range:

ğŸ‘‰ **$2K â€“ $10K** depending on pricing tier and preprocessing.

---

# 4ï¸âƒ£ Inference Cost (Post Fine-Tuning)

Assume:

* Avg prompt: 2,000 tokens
* Avg output: 500 tokens
* 10,000 queries per month

Tokens per query:

```
2,500 tokens
```

Monthly tokens:

```
2,500 Ã— 10,000 = 25M tokens
```

Inference cost depends on:

* Model size
* Input vs output pricing

But expect **hundreds to low thousands per month** at scale.

---

# 5ï¸âƒ£ Important Real-World Consideration

Training on **1B tokens is massive**.

Most financial fine-tuning setups:

* Use 50Mâ€“300M tokens
* Or use RAG instead of full fine-tuning
* Or LoRA adapters on open weights

For SEC filings specifically:

ğŸ‘‰ RAG + small instruction tuning often beats brute-force full corpus training.

---

# 6ï¸âƒ£ Quick Mental Model

| Corpus Size    | Tokens       | Scale      |
| -------------- | ------------ | ---------- |
| 1 filing       | ~100k tokens | small      |
| 100 filings    | ~10M tokens  | medium     |
| 10,000 filings | ~1B tokens   | very large |

---

If you'd like, I can next:

* Show Python code to compute exact token counts using `tiktoken`
* Estimate cost for specific OpenAI model pricing
* Compare cost of RAG vs fine-tuning
* Show optimal chunking strategy for 10-Ks


