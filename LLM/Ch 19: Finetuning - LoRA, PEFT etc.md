Hereâ€™s a clean, structured map of **fine-tuning techniques** used for LLMs.

Weâ€™ll group them by **how much of the model you update**.

---

# ðŸ§± 1ï¸âƒ£ Full Fine-Tuning (Dense FT)

Update **all parameters** of the model.

```
W â† W - Î· âˆ‡L
```

### âœ… Pros

* Maximum flexibility
* Best domain adaptation

### âŒ Cons

* Very expensive (VRAM heavy)
* Risk of catastrophic forgetting

Used when:

* You own large compute
* Need deep domain shift

---

# ðŸ§© 2ï¸âƒ£ Parameter-Efficient Fine-Tuning (PEFT)

Update only a **small subset of parameters**.

Much cheaper.

---

## ðŸ”¹ LoRA (Low-Rank Adaptation)

Add low-rank matrices:

```
W' = W + AÂ·B
```

Only train A and B.

### Why it works:

Weight updates are often low-rank.

Most popular PEFT method.

---

## ðŸ”¹ QLoRA

LoRA + 4-bit quantization.

* Base model in 4-bit
* LoRA layers in higher precision
* Extremely memory efficient

Allows 65B models on single GPU.

---

## ðŸ”¹ Adapters

Insert small MLP layers between transformer blocks.

Only adapters are trained.

More parameters than LoRA, but simple.

---

## ðŸ”¹ Prefix Tuning

Instead of changing weights:

Learn virtual tokens prepended to input.

Model stays frozen.

Useful for lightweight customization.

---

## ðŸ”¹ Prompt Tuning

Learn continuous embeddings that act like prompts.

Even smaller than prefix tuning.

Very lightweight.

---

# ðŸŽ¯ 3ï¸âƒ£ Supervised Fine-Tuning (SFT)

Train on:

```
Instruction â†’ Ideal Answer
```

Used for:

* Chat alignment
* Domain adaptation
* Structured output learning

Often combined with LoRA.

---

# ðŸ§  4ï¸âƒ£ Reinforcement Learning-Based Methods

After SFT.

---

## ðŸ”¹ RLHF (Reinforcement Learning from Human Feedback)

* Train reward model
* Optimize policy via PPO

Complex, expensive.

---

## ðŸ”¹ DPO (Direct Preference Optimization)

No reward model.
Optimizes preference pairs directly.

Simpler and now widely used.

---

# ðŸ§® 5ï¸âƒ£ Continual / Domain Adaptive Pretraining

Before SFT.

Train on domain corpus with next-token objective.

Example:

* Finance documents
* Legal corpus
* Medical text

Improves base knowledge before alignment.

---

# ðŸ”€ 6ï¸âƒ£ Multi-Task Fine-Tuning

Train on multiple tasks simultaneously.

Helps:

* Generalization
* Avoid overfitting

---

# ðŸ§  7ï¸âƒ£ Instruction Tuning

Large-scale SFT across many tasks.

Improves zero-shot performance.

---

# ðŸ“Š Comparison Table

| Method        | Train Params | Compute   | Flexibility       |
| ------------- | ------------ | --------- | ----------------- |
| Full FT       | 100%         | Very High | Max               |
| LoRA          | ~0.1â€“1%      | Low       | High              |
| QLoRA         | ~0.1%        | Very Low  | High              |
| Adapters      | Small        | Medium    | High              |
| Prefix        | Tiny         | Very Low  | Medium            |
| Prompt tuning | Tiny         | Very Low  | Lower             |
| DPO           | Small        | Medium    | Alignment-focused |

---

# ðŸŽ¯ Practical Guidance

If youâ€™re building production systems:

* ðŸŸ¢ Domain adaptation â†’ LoRA or QLoRA
* ðŸŸ¢ Chat alignment â†’ SFT + DPO
* ðŸŸ¢ Low-resource GPU â†’ QLoRA
* ðŸ”µ Large enterprise model â†’ Full FT + RLHF
* ðŸŸ¡ Lightweight customization â†’ Prefix/Prompt tuning

---

# ðŸ§  Mental Model

Fine-tuning spectrum:

```
Prompt tuning  â†’  LoRA  â†’  Full FT
   (tiny)         (small)    (massive)
```

Tradeoff = flexibility vs cost.

---

If you'd like, I can next explain:

* When RAG is better than fine-tuning
* How to choose LoRA rank
* Hyperparameter strategies
* Catastrophic forgetting
* Fine-tuning for financial LLM systems


Hereâ€™s the **real-world usage picture (2025)** â€” whatâ€™s actually widely used in industry and open-source.

---

# ðŸ¥‡ Most Widely Used Today

## 1ï¸âƒ£ LoRA (and QLoRA) â†’ â­â­â­â­â­

**Most common fine-tuning method overall.**

Used for:

* Domain adaptation
* Instruction tuning
* Enterprise customization
* Open-source models (LLaMA-family, Mistral, etc.)

Why it dominates:

* Cheap
* Stable
* Easy to implement
* Works very well

QLoRA is especially popular because:

* 4-bit base model
* Train on single GPU
* Excellent cost/performance

ðŸ‘‰ If someone says â€œwe fine-tuned a model,â€
very often it means **LoRA**.

---

## 2ï¸âƒ£ Supervised Fine-Tuning (SFT) â†’ â­â­â­â­â­

Nearly universal.

Every production chat model goes through SFT.

Used for:

* Chat formatting
* Structured outputs
* Instruction following
* Domain tone alignment

Even when people say â€œRLHF model,â€ it *still started with SFT*.

---

# ðŸ¥ˆ Widely Used but More Specialized

## 3ï¸âƒ£ DPO (Direct Preference Optimization) â†’ â­â­â­â­

Now very common in:

* Open-source alignment
* Mid-size companies
* Preference alignment workflows

Replacing PPO-based RLHF in many pipelines because:

* Simpler
* More stable
* Cheaper

---

## 4ï¸âƒ£ Domain-Adaptive Pretraining â†’ â­â­â­

Used in:

* Legal
* Finance
* Medical
* Code models

Large enterprises do this more than startups.

---

# ðŸ¥‰ Less Common (Production)

## 5ï¸âƒ£ Full Fine-Tuning â†’ â­â­

Used mostly by:

* Big AI labs
* Large enterprises
* When massive compute available

Expensive and risky (catastrophic forgetting).

---

## 6ï¸âƒ£ RLHF with PPO â†’ â­â­ (declining)

Still used at frontier labs.

But many organizations moved to DPO or simpler alignment methods.

Complex and compute-heavy.

---

## 7ï¸âƒ£ Adapters â†’ â­â­

Still used, but LoRA largely replaced them.

---

## 8ï¸âƒ£ Prefix / Prompt Tuning â†’ â­

Used in research.
Rare in serious production LLM systems.

LoRA performs better in practice.

---

# ðŸ“Š Realistic Ranking (Industry View)

| Technique            | Industry Usage        |
| -------------------- | --------------------- |
| SFT                  | Extremely common      |
| LoRA                 | Extremely common      |
| QLoRA                | Extremely common      |
| DPO                  | Growing fast          |
| Domain Pretraining   | Moderate              |
| Full FT              | Limited (big players) |
| PPO-based RLHF       | Frontier labs         |
| Prefix/Prompt tuning | Rare                  |

---

# ðŸŽ¯ What Most Companies Actually Do

Typical modern stack:

```
Base Model
   â†“
Domain-Adaptive Pretraining (optional)
   â†“
SFT (via LoRA/QLoRA)
   â†“
DPO alignment
```

Thatâ€™s the dominant pattern.

---

# ðŸ§  Key Insight

The industry shifted toward:

> **Parameter-efficient + simple optimization**

Because:

* GPUs are expensive
* Stability matters
* Iteration speed matters
* Scaling LoRA is easy

---

If you'd like, I can next explain:

* What OpenAI likely uses internally
* What Meta uses for LLaMA
* What startups vs banks typically deploy
* When RAG replaces fine-tuning
* Cost comparison of LoRA vs full FT
