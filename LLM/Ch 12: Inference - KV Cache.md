### ðŸ”Ž Inference I: KV-Cache (Keyâ€“Value Cache)

When a GPT-style transformer generates text **token by token**, it repeatedly uses self-attention.
Without optimization, every new token would recompute attention over *all previous tokens* â€” very expensive.

**KV-cache solves this.**

---

## ðŸ§  Quick Intuition

During inference:

1. Each token produces:

   * **Q** (Query)
   * **K** (Key)
   * **V** (Value)

2. For the *next* token:

   * We only need a new **Q**
   * Previous **K and V donâ€™t change**

So instead of recomputing K and V for all prior tokens, we:

> âœ… **Cache K and V once**
> âžœ Reuse them for every next token

Thatâ€™s the **KV-cache**.

---

## ðŸ“¦ What Exactly Is Cached?

For each transformer layer:

```
Layer L:
  K_cache: [t1, t2, t3, ..., tn]
  V_cache: [t1, t2, t3, ..., tn]
```

Stored per layer, per head.

So total memory:

```
O(layers Ã— heads Ã— sequence_length Ã— head_dim)
```

This is why long context = large memory usage.

---

## âš¡ Why It Matters

Without KV-cache:

```
Cost per token â‰ˆ O(nÂ²)
```

With KV-cache:

```
Cost per new token â‰ˆ O(n)
```

Where `n` = sequence length so far.

This is what makes fast autoregressive decoding possible.

---

## ðŸ— Step-by-Step During Inference

Assume prompt:

> "Hello world"

### Step 1 â€” Process prompt

* Compute Q, K, V for all tokens
* Store all K and V in cache

### Step 2 â€” Generate next token

* Compute Q for new token only
* Use:

  ```
  attention(Q_new, K_cached, V_cached)
  ```
* Append new K and V to cache

Repeat.

---

## ðŸ”¥ Why KV-Cache Is Inference-Only

During **training**:

* We process full sequences in parallel
* No need for cache

During **inference**:

* We generate sequentially
* Cache avoids recomputation

---

## ðŸ§® Memory Tradeoff

KV-cache trades:

| Saves   | Costs      |
| ------- | ---------- |
| Compute | GPU memory |

Large context windows (e.g., 128k tokens) are expensive mainly because of KV-cache storage.

---

## ðŸŽ¯ Simple Mental Model

Think of it like:

> ðŸ“’ Instead of re-reading the whole book every time you write the next sentence,
> you keep notes (K,V) from previous pages and only add new notes.

---

If you'd like, I can next explain:

* KV-cache memory calculation example (with numbers)
* Paged KV-cache (used in vLLM)
* FlashAttention interaction
* Multi-query / grouped-query attention optimization
