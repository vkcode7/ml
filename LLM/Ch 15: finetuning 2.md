## ğŸ¯ Fine-Tuning II: Reinforcement Learning (RL, RLHF, PPO, DPO)

After **SFT** teaches the model *how to answer*,
**RL fine-tuning** teaches it *which answers humans prefer*.

Think:

> SFT â†’ correctness
> RL â†’ preference alignment

---

# ğŸ§  Why RL Is Needed

SFT learns from:

```
Prompt â†’ Ideal Answer
```

But many tasks donâ€™t have a single correct answer.

Example:

* More polite vs less polite
* More concise vs verbose
* Safer vs risky
* Financial explanation: retail-friendly vs quant-heavy

Human preference is subjective.

Thatâ€™s where **Reinforcement Learning from Human Feedback (RLHF)** comes in.

---

# ğŸ”¥ RLHF (Reinforcement Learning from Human Feedback)

Pipeline:

### 1ï¸âƒ£ SFT model

Start from supervised fine-tuned model.

### 2ï¸âƒ£ Collect preference data

Humans compare two outputs:

```
Prompt: Explain convexity.

Answer A: ...
Answer B: ...
Human: prefers B
```

### 3ï¸âƒ£ Train a Reward Model

Model learns:

```
R(prompt, answer) â†’ scalar score
```

### 4ï¸âƒ£ Optimize policy with RL

Update LLM to maximize reward.

---

# ğŸ§® PPO (Proximal Policy Optimization)

Most famous RLHF algorithm.

Used in early ChatGPT systems.

### Objective:

Maximize:

```
Expected reward
```

But prevent model from drifting too far from SFT.

So we optimize:

```
Reward - KL penalty
```

Where:

* Reward = reward model score
* KL penalty = distance from original SFT model

This keeps model stable.

---

### ğŸ§  Intuition

Without KL penalty:

* Model may exploit reward model
* Produce weird but high-reward outputs

With KL penalty:

* Stays close to base personality

---

# âš ï¸ PPO Challenges

* Complex training
* Two models (policy + reward)
* RL instability
* Expensive
* Sensitive hyperparameters

---

# ğŸš€ DPO (Direct Preference Optimization)

Modern alternative.

Much simpler.

Instead of:

* Training reward model
* Running PPO

DPO directly optimizes preference pairs.

---

## DPO Idea

Given:

```
Prompt
Chosen answer (preferred)
Rejected answer
```

Optimize model so:

```
P(chosen | prompt) > P(rejected | prompt)
```

With closed-form objective.

No reward model.
No RL loop.
No PPO.

---

# ğŸ“Š PPO vs DPO

| Feature        | PPO (RLHF) | DPO    |
| -------------- | ---------- | ------ |
| Reward model   | Yes        | No     |
| RL loop        | Yes        | No     |
| Stability      | Hard       | Easier |
| Compute cost   | High       | Lower  |
| Implementation | Complex    | Simple |

DPO is now widely preferred for open models.

---

# ğŸ— Conceptual Math View

### PPO optimizes:

```
max E[R] - Î² KL(Ï€ || Ï€_ref)
```

### DPO optimizes:

```
log Ïƒ(Î² (log Ï€(chosen) - log Ï€(rejected)))
```

Much cleaner.

---

# ğŸ§  Big Picture of Fine-Tuning Stages

```
Pretraining â†’ language knowledge
SFT â†’ instruction following
RLHF / DPO â†’ alignment to human preferences
```

---

# ğŸ¯ What RL Fine-Tuning Actually Changes

* Politeness
* Helpfulness
* Safety boundaries
* Conciseness vs verbosity
* Tone
* Refusal behavior
* Reasoning style

---

# ğŸ§  Mental Model

If SFT teaches:

> â€œHow to write answersâ€

RL teaches:

> â€œWhich answers humans like bestâ€

---

If you'd like next, I can explain:

* Why reward hacking happens
* How KL divergence prevents collapse
* Why DPO often matches PPO in practice
* Alignment vs capability distinction
* How this applies to domain LLMs (e.g., fixed income chat models)
